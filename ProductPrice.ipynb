{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cfabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "201ce7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python funtion to get all product links from walmart\n",
    "headers = { \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"}\n",
    "\n",
    "result = []\n",
    "\n",
    "def walmart(name):\n",
    "    Walmart = 'https://www.walmart.com/search?q='\n",
    "    page = Request(Walmart + name, headers=headers)\n",
    "    html = urlopen(page).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    results = soup.find_all('a', class_=\"absolute w-100 h-100 z-1\" )\n",
    "\n",
    "    try: \n",
    "        for link in results:\n",
    "            if 'href' in link.attrs and 'https' in link.attrs['href']:\n",
    "                result.append(str(link.attrs['href']))\n",
    "                \n",
    "            else:\n",
    "                result.append('https://www.walmart.com/' + str(link.attrs['href']))\n",
    "\n",
    "                \n",
    "                        \n",
    "    except AttributeError:\n",
    "            print(\"title: -\")\n",
    "\n",
    "\n",
    "#CREATE a function to scrap the product name, price and product image link from Walmart\n",
    "final_list = []\n",
    "\n",
    "def get_items_price(urls):\n",
    "\n",
    "    try:\n",
    "        for url in urls:\n",
    "            pages = Request(url, headers=headers)\n",
    "            html = urlopen(pages).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            title = soup.find(class_=\"f3 b lh-copy dark-gray mt1 mb2\").get_text()\n",
    "            price = soup.find(itemprop=\"price\").get_text()\n",
    "\n",
    "            for item in soup.find_all('img', loading=\"lazy\"):\n",
    "                    \n",
    "                images = item['src']\n",
    "            items = title, price, images\n",
    "\n",
    "            final_list.append(items)\n",
    "                        \n",
    "    except AttributeError:\n",
    "        print(\"title: -\")\n",
    "        \n",
    "    return \n",
    "\n",
    "walmart('ProductName')\n",
    "\n",
    "get_items_price(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5236d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for the product name, price and the product image link \n",
    "import urllib\n",
    "\n",
    "df=pd.DataFrame(final_list, columns =['Item name', 'Price', 'Item image link'])\n",
    "\n",
    "#add the product link to the dataframe\n",
    "df['Item link'] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53495af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: -\n"
     ]
    }
   ],
   "source": [
    "# Create a python funtion to get all product links from Amazon\n",
    "\n",
    "headers = { \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"}\n",
    "\n",
    "result = []\n",
    "\n",
    "def amazon(name):\n",
    "    Walmart = 'https://www.amazon.com/s?k='\n",
    "    page = Request(Walmart + name, headers=headers)\n",
    "    html = urlopen(page).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    results = soup.find_all('a', class_=\"a-link-normal s-no-outline\" )\n",
    "\n",
    "    try: \n",
    "        for link in results:\n",
    "            if 'href' in link.attrs and 'https' in link.attrs['href']:\n",
    "                result.append(str(link.attrs['href']))\n",
    "                \n",
    "            else:\n",
    "                result.append('https://www.amazon.com/' + str(link.attrs['href']))\n",
    "\n",
    "                \n",
    "                        \n",
    "    except AttributeError:\n",
    "            print(\"title: -\")\n",
    "\n",
    "\n",
    "amazon('ProductName')\n",
    "\n",
    "#CREATE a function to scrap the product name, price and product image link from Amazon\n",
    "\n",
    "final_list = []\n",
    "\n",
    "def get_items_price(urls):\n",
    "\n",
    "    try:\n",
    "        for url in urls:\n",
    "            pages = Request(url, headers=headers)\n",
    "            html = urlopen(pages).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            title = soup.find(id=\"productTitle\").get_text().strip()\n",
    "#             print(title)\n",
    "            price = soup.find(id=\"priceblock_ourprice\").get_text()\n",
    "#             print(price)\n",
    "\n",
    "            for item in soup.find_all('img', attrs={\"id\":'landingImage'}):                    \n",
    "                images = item['src']\n",
    "#                 print(images)\n",
    "            \n",
    "#             print(title, price, images)\n",
    "            items = title, price, images\n",
    "\n",
    "            final_list.append(items)\n",
    "                        \n",
    "    except AttributeError:\n",
    "        print(\"title: -\")\n",
    "        \n",
    "    return \n",
    "\n",
    "get_items_price(result)\n",
    "\n",
    "# print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8a295e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "amz=pd.DataFrame(final_list, columns =['Item name', 'Price', 'Item image link'])\n",
    "\n",
    "amz['Item link'] = pd.Series(result)\n",
    "xlsx_file = amz.to_excel('ProductName.xlsx', index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96ae409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the two dataframes from Walmart and Amazon together\n",
    "pieces= {'Walmart': df, 'Amazon': amz}\n",
    "\n",
    "result = pd.concat(pieces)\n",
    "\n",
    "#Export the data frame into an excel file\n",
    "xlsx_file = result.to_excel('ProductName.xlsx', index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c9ae0c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use python function to send the excel file to an email address\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "\n",
    "SENDER_EMAIL = \"youremail@gmail.com\"\n",
    "APP_PASSWORD = \"yourPassword\"\n",
    "\n",
    "def send_mail_with_excel(recipient_email, subject, content, excel_file):\n",
    "    msg = EmailMessage()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = SENDER_EMAIL\n",
    "    msg['To'] = recipient_email\n",
    "    msg.set_content(content)\n",
    "\n",
    "    with open(excel_file, 'rb') as f:\n",
    "        file_data = f.read()\n",
    "    msg.add_attachment(file_data, maintype=\"application\", subtype=\"xlsx\", filename=excel_file)\n",
    "\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
    "        smtp.login(SENDER_EMAIL, APP_PASSWORD)\n",
    "        smtp.send_message(msg)\n",
    "        \n",
    "recipient = 'yourRecipientemail@gmail.com'\n",
    "subject = 'subject'\n",
    "content = 'content'\n",
    "send_mail_with_excel(recipient, subject, content, 'ProductName.xlsx' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
